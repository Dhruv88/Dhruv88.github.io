---
title: "Distributed Graph Neural Networks"
collection: research-projects
permalink: /research-projects/2023-01-01-research-project-4
excerpt: 'Scalable and distributed efficient training of GNNs'
dateFrom: 2023-01-01
dateTo: 2023-07-01
mentor: "Dr. Gagan Raj Gupta and Dr. Vishwesh Jatala"
# venue: 'Journal 1'
# paperurl: 'http://academicpages.github.io/files/paper1.pdf'
# citation: 'Your Name, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---

Several distributed frameworks have been developed to scale Graph Neural Networks (GNNs) on billion-size graphs. On several benchmarks, we observe that the graph partitions generated by these frameworks have heterogeneous data distributions and class imbalance, affecting convergence, and resulting in lower performance than centralized implementations. We holistically address these challenges and develop techniques that reduce training time and improve accuracy. We develop an Edge-Weighted partitioning technique to improve the micro average F1 score (accuracy) by minimizing the total entropy. Furthermore, we add an asynchronous personalization phase that adapts each compute-host's model to its local data distribution. We design a class-balanced sampler that considerably speeds up convergence. We implemented our algorithms on the DistDGL framework and observed that our training techniques scale much better than the existing training approach. We achieved a (2-3x) speedup in training time and 4\% improvement on average in micro-F1 scores on 5 large graph benchmarks compared to the standard baselines.

## The paper has been accepted at ICDM'23(paper link will be updated soon)
[Paper](https://arxiv.org/abs/2311.02399)
[Code](https://github.com/Anirban600/EAT-DistGNN)
