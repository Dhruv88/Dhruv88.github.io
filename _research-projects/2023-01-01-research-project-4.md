---
title: "Distributed Graph Neural Networks"
collection: research-projects
permalink: /research-projects/2023-01-01-research-project-4
excerpt: 'Scalable and distributed efficient training of GNNs'
dateFrom: 2023-01-01
dateTo: 2023-07-01
mentor: "Dr. Gagan Raj Gupta and Dr. Vishwesh Jatala"
venue: 'International conference on Data Mining(ICDM) 2023'
paperurl: 'https://arxiv.org/abs/2311.02399'
citation: 'D. Deshmukh, G. R. Gupta, M. Chawla, V. Jatala and A. Haldar, "Entropy Aware Training for Fast and Accurate Distributed GNN," 2023 IEEE International Conference on Data Mining (ICDM), Shanghai, China, 2023, pp. 986-991, doi: 10.1109/ICDM58522.2023.00112.
keywords: {Training;Benchmark testing;Graph neural networks;Entropy;Partitioning algorithms;Standards;Convergence;Distributed ML;graph neural networks;class imbalance},'
---

![Our techniques](../images/image15.png)

Several distributed frameworks have been developed to scale Graph Neural Networks (GNNs) on billion-size graphs. On several benchmarks, we observe that the graph partitions generated by these frameworks have heterogeneous data distributions and class imbalance, affecting convergence, and resulting in lower performance than centralized implementations. We holistically address these challenges and develop techniques that reduce training time and improve accuracy. We develop an Edge-Weighted partitioning technique to improve the micro average F1 score (accuracy) by minimizing the total entropy. Furthermore, we add an asynchronous personalization phase that adapts each compute-host's model to its local data distribution. We design a class-balanced sampler that considerably speeds up convergence. We implemented our algorithms on the DistDGL framework and observed that our training techniques scale much better than the existing training approach. We achieved a (2-3x) speedup in training time and 4\% improvement on average in micro-F1 scores on 5 large graph benchmarks compared to the standard baselines.

[Code](https://github.com/Anirban600/EAT-DistGNN)
